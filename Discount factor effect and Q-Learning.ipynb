{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Discount factor effect and Q-Learning"
      ],
      "metadata": {
        "id": "Uj3ZwlkRQKpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Based on the probabilities on the arrows above, Model your MDP (transition probabilities and rewards) in the notebook. you can use the [s, a, sâ€™] for each part or you can use your own way of defining the MDP. What should be seen are transition probabilities, rewards and possible actions"
      ],
      "metadata": {
        "id": "JUL_pNnRBOb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = {\n",
        "    (0, 0): [(0, 0.7), (1, 0.3)],\n",
        "    (0, 1): [(0, 1.0)],\n",
        "    (0, 2): [(0, 0.8), (1, 0.2)],\n",
        "    (1, 0): [(1, 1)],\n",
        "    (1, 2): [(2, 1.0)],\n",
        "    (2, 1): [(2, 0.1), (1, 0.1), (0, 0.8)],\n",
        "}\n",
        "\n",
        "rewards = {\n",
        "    (0, 0, 0): 10,\n",
        "    (1, 2, 2): -50,\n",
        "    (2, 1, 0): 40,\n",
        "}\n",
        "\n",
        "possible_actions = [[0, 1, 2], [0, 2], [1]]"
      ],
      "metadata": {
        "id": "gQ8nWasjQK-x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "JkFqkJCuQPOi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state, actions] = 0.0  # for all possible actions"
      ],
      "metadata": {
        "id": "-mcUQqriQLoW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Take your discount factor to be 0.9. Perform Q-learning and report the Q-values for each (state, action) pair. Based on that, what is the optimal policy?"
      ],
      "metadata": {
        "id": "y1aW8WF16GOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.9  # the discount factor\n",
        "#gamma = 0.95\n",
        "for iteration in range(50):\n",
        "    for state in range(3):\n",
        "      for action in possible_actions[state]:\n",
        "          updated_q_value = 0\n",
        "          for next_state, prob in transition_probabilities[(state, action)]:\n",
        "              reward = rewards.get((state, action, next_state), 0)\n",
        "              max_q_next = max(Q_values[next_state])\n",
        "              updated_q_value += prob * (reward + gamma * max_q_next)\n",
        "\n",
        "          Q_values[state, action] = updated_q_value"
      ],
      "metadata": {
        "id": "zx3UhYrBQNiu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "id": "gdRbvTjMQTWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b581e9f-34e3-49c9-e5d0-95c8c1d64e7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18.91891892, 17.02702703, 13.62162162],\n",
              "       [ 0.        ,        -inf, -4.87971488],\n",
              "       [       -inf, 50.13365013,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values.argmax(axis=1)  # optimal action for each state"
      ],
      "metadata": {
        "id": "lGHSyhS3QV_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5054694-b556-46d2-d7b5-8b9cdf04d7c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Perform the same procedure but this time with a discount factor of 0.95. Did your optimal policy change? Explain your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "cyS6xw6MCu8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "for state, actions in enumerate(possible_actions):\n",
        "    Q_values[state, actions] = 0.0  # for all possible actions"
      ],
      "metadata": {
        "id": "BVVJtMcoBZ7c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "id": "0xwcXCskC0aD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67694bbf-51d4-44e7-c7db-6f620e90b920"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.,   0.,   0.],\n",
              "       [  0., -inf,   0.],\n",
              "       [-inf,   0., -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use the same code as above\n",
        "# gamma = 0.9  # the discount factor\n",
        "gamma = 0.95\n",
        "for iteration in range(50):\n",
        "    for state in range(3):\n",
        "      for action in possible_actions[state]:\n",
        "          updated_q_value = 0\n",
        "          for next_state, prob in transition_probabilities[(state, action)]:\n",
        "              reward = rewards.get((state, action, next_state), 0)\n",
        "              max_q_next = max(Q_values[next_state])\n",
        "              updated_q_value += prob * (reward + gamma * max_q_next)\n",
        "\n",
        "          Q_values[state, action] = updated_q_value"
      ],
      "metadata": {
        "id": "Qgiu3B2OU7yf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrQYY8T6U-kq",
        "outputId": "ab518f3a-2dc3-4c4d-afb9-c5f09df71ebb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21.79615996, 20.70635196, 16.76923123],\n",
              "       [ 1.02074831,        -inf,  1.08097586],\n",
              "       [       -inf, 53.77587186,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values.argmax(axis=1)  # optimal action for each state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E70zXaxPU_VI",
        "outputId": "4739fcba-a4b7-4283-fc3d-cdb2c3f36a0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}